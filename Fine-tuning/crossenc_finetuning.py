# -*- coding: utf-8 -*-
"""crossenc_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bHfobdlTKA3FoRYeKIaWKmChrHWCEB_b
"""

import os
import ast
import gc
import faiss
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from pathlib import Path
from huggingface_hub import login
from sentence_transformers import CrossEncoder, InputExample, LoggingHandler
from sentence_transformers.cross_encoder.evaluation import CrossEncoderClassificationEvaluator
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from datasets import Dataset
from huggingface_hub import HfApi, login

tydi_train = pd.read_csv("tydiqa_arabic_train.csv", sep='\t', encoding="utf-8")
tafseer_train = pd.read_csv("tafsir_train.csv")
quqa_train = pd.read_csv("train_quqa.csv")
haqa_train = pd.read_csv("haqa_train.csv")
task_train = pd.read_csv("training_neg.csv")

def standardize_dataset_format(df: pd.DataFrame, dataset_name: str = "unknown") -> pd.DataFrame:
    """
    Standardize different datasets:
    - text1: question text
    - text2: passage text
    - label: relevance label
    - passage_id: unique passage identifier
    """

    print(f"Standardizing dataset: {dataset_name}")
    print(f"Original shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")

    standardized_data = []

    if 'question' in df.columns and 'passage' in df.columns:
        for _, row in df.iterrows():
            standardized_data.append({
                "text1": str(row["question"]).strip(),
                "text2": str(row["passage"]).strip(),
                "label": int(row["label"]),
                "passage_id": str(row.get("passage_id", row.get("question_id", f"unknown_{len(standardized_data)}")))
            })

    elif 'question' in df.columns and 'passage_id' in df.columns:
        print("This format requires separate passage lookup!")
        return df

    else:
        raise ValueError(f"Unsupported dataset format. Columns: {list(df.columns)}")

    result_df = pd.DataFrame(standardized_data)
    print(f"Standardized shape: {result_df.shape}")
    print(f"Positive samples: {(result_df['label'] == 1).sum()}")
    print(f"Negative samples: {(result_df['label'] == 0).sum()}")

    return result_df

tydi_train = standardize_dataset_format(tydi_train, "tydi")
tafseer_train = standardize_dataset_format(tafseer_train, "tafseer")
quqa_train = standardize_dataset_format(quqa_train, "quqa")
haqa_train = standardize_dataset_format(haqa_train, "haqa")
task_train = standardize_dataset_format(task_train, "task")

def fine_tune(
    df: pd.DataFrame,
    model_name: str = "aubmindlab/bert-base-arabertv02",
    output_dir: str = "finetuned_cross_encoder/",
    batch_size: int = 16,
    epochs: int = 3,
    learning_rate: float = 1e-6,
    warmup_steps: int = 100,
    show_progress_bar: bool = True,
    ft_model: CrossEncoder = None,
    hf_org: str = "yoriis",
    checkpoint_name: str = "checkpoint"
):
    print(f"\nFine-tuning CROSS-ENCODER `{model_name}` on {len(df)} samples")

    train_df, eval_df = train_test_split(df, test_size=0.1, stratify=df['label'] > 0, random_state=42)

    train_samples = [
        InputExample(texts=[row["text1"], row["text2"]], label=float(row["label"]))
        for _, row in train_df.iterrows()
    ]

    eval_samples = [
        InputExample(texts=[row["text1"], row["text2"]], label=float(row["label"]))
        for _, row in eval_df.iterrows()
    ]

    evaluator = CrossEncoderClassificationEvaluator(
        [ex.texts for ex in eval_samples],
        [ex.label for ex in eval_samples],
        name="eval"
    )

    model = ft_model if ft_model else CrossEncoder(model_name, num_labels=1)
    os.makedirs(output_dir, exist_ok=True)

    model.fit(
        train_dataloader=DataLoader(train_samples, shuffle=True, batch_size=batch_size),
        evaluator=evaluator,
        epochs=epochs,
        evaluation_steps=500,
        warmup_steps=warmup_steps,
        output_path=output_dir,
        optimizer_params={'lr': learning_rate},
        show_progress_bar=show_progress_bar,
        use_amp=True,
    )

    model.save(output_dir)

    print(f"CrossEncoder saved and pushed as `{checkpoint_name}`")

    return model

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

HF_ORG = "HF_ORG"
HF_TOKEN = "HF_TOKEN"

login(token=HF_TOKEN)

def train_model():
  i = 1

  # TYDI
  model = fine_tune(tydi_train, model_name = "BASE_MODEL_NAME", output_dir=f"finetuned/step_{i}/")
  model.push_to_hub(f"{HF_ORG}/MODEL_NAME-tydi")

  i += 1

  # TAFSEER
  model = fine_tune(tafseer_train, model_name = "MODEL_NAME", output_dir=f"finetuned/step_{i}/", ft_model = model)
  model.push_to_hub(f"{HF_ORG}/MODEL_NAME-tafseer")

  i += 1

  # QUQA
  model = fine_tune(quqa_train, model_name = "MODEL_NAME", output_dir=f"finetuned/step_{i}/", ft_model = model)
  model.push_to_hub(f"{HF_ORG}/MODEL_NAME-quqa")

  i += 1

  # HAQA
  model = fine_tune(haqa_train, model_name = "MODEL_NAME", output_dir=f"finetuned/step_{i}/", ft_model = model)
  model.push_to_hub(f"{HF_ORG}/MODEL_NAME-haqa")

  return model